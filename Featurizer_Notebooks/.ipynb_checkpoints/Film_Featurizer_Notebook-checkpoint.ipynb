{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Modeling and Data Analysis - LD,75 Modeling Paper\n",
    "\n",
    "This notebook is a version of earlier modeling efforts, used for a very specific dataset for a paper that will be submitted imminently to ACSEL? EES? Adv. Mater.? This will be considered the authoritative guide to which data will be considered for use in this effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports as well as some additional ones for machine learning, plotting etc.\n",
    "# farther down the cell are some functions \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.stats\n",
    "from scipy.integrate import trapz\n",
    "import scipy as sp\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from os import listdir\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import SQ_calcs\n",
    "import json\n",
    "import os\n",
    "%matplotlib inline\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import seaborn\n",
    "import itertools\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import metrics\n",
    "import datetime as dt\n",
    "\n",
    "from skimage.io import imread\n",
    "from matplotlib import transforms\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#change default plot settings\n",
    "mpl.style.use('wiley_publication.mplstyle')\n",
    "plt.rc('xtick',labelsize=20)\n",
    "plt.rc('ytick',labelsize=20)\n",
    "\n",
    "## Useful functions defined below:\n",
    "def bleach_rate_from_pct_increase(threshold,t,Tr):\n",
    "# arguments: threshold: relative increase in norm. transmittance used to define bleaching rate\n",
    "#            t: time series, in minutes\n",
    "#            Tr: normalized transmittance timeseries\n",
    "#\n",
    "    # first find index at which normalized transmittance increases beyond threshold\n",
    "    for kk in range(len(Tr)):\n",
    "        if Tr[kk] > threshold:\n",
    "            break\n",
    "    \n",
    "    # if the loop gets to the end without crossing the threshold, assign NaN value to the bleaching rate\n",
    "    if kk == len(Tr)-1:\n",
    "        tau = np.nan\n",
    "    else:\n",
    "    # do linear interpolation between points just above and just below threshold to get \n",
    "    # \"failure\" time at exactly the threshold\n",
    "        tau = t[kk-1] + (t[kk]-t[kk-1])*(threshold-Tr[kk-1])/(Tr[kk]-Tr[kk-1])\n",
    "    \n",
    "    # invert \"failure time\" to get bleaching rate, in 1/min\n",
    "    bleach_rate = (threshold-1)/tau\n",
    "    \n",
    "    # return the bleaching rate\n",
    "    return bleach_rate\n",
    "\n",
    "# alternative way to extract bleaching rate: instead of taking secant approximation to degradation rate, \n",
    "# perform a linear fit to all the data up to the threshold and ignore everything else;\n",
    "# if the threshold can't be reached, just fit the entire dataset\n",
    "def bleach_rate_from_linear_fit_to_pct_increase(threshold,t,Tr):\n",
    "# arguments: threshold: relative increase in norm. transmittance used to define belaching rate\n",
    "#            t: time series, in minutes\n",
    "#            Tr: normalized transmittance timeseries\n",
    "#\n",
    "    # first find index at which normalized transmittance increases beyond threshold\n",
    "    for kk in range(len(Tr)):\n",
    "        if Tr[kk]/Tr[0] > threshold:\n",
    "            break\n",
    "    \n",
    "    # use linear polynomial fit up to threshold\n",
    "    coeffs = np.polyfit(t[:kk+1],Tr[:kk+1],1)\n",
    "    bleach_rate = coeffs[0]\n",
    "    intercept = coeffs[1]\n",
    "    \n",
    "    # return the bleaching rate\n",
    "    return bleach_rate, intercept\n",
    "\n",
    "\n",
    "# define function for handling marker styles when plotting to reflect \n",
    "# the environmental conditions used for a given data point\n",
    "def envt_plot_style(temp,rh,o2,illum,encap):\n",
    "    if temp < 15:\n",
    "        m_color = np.array([0.00,0.00,0.20])\n",
    "    elif 15 < temp <= 35:\n",
    "        m_color = np.array([0.00,0.20,0.20])\n",
    "    elif 35 < temp <= 55:\n",
    "        m_color = np.array([0.00,0.20,0.00])\n",
    "    elif 55 < temp <= 75:\n",
    "        m_color = np.array([0.20,0.20,0.00])\n",
    "    elif 75 < temp:\n",
    "        m_color = np.array([0.20,0.00,0.00])\n",
    "\n",
    "    # humidity encoded by brightness\n",
    "    if rh < 10:\n",
    "        m_color *= 1\n",
    "    elif 10 < rh < 30:\n",
    "        m_color *= 2\n",
    "    elif 30 < rh < 50:\n",
    "        m_color *= 3\n",
    "    elif 50 < rh < 70:\n",
    "        m_color *= 4\n",
    "    elif 70 < rh:\n",
    "        m_color *= 5\n",
    "\n",
    "    # illumination encoded by marker shape\n",
    "    if illum == 0:\n",
    "        m_shape = 'p' # pentagons\n",
    "    elif illum <= 0.5:\n",
    "        m_shape = '^' # upright triangles\n",
    "    elif illum <= 1:\n",
    "        m_shape = 'v' # inverted triangles\n",
    "    elif illum <= 2:\n",
    "        m_shape = '8' # octagons\n",
    "    elif illum <= 4:\n",
    "        m_shape = 'h' # hexagons\n",
    "    elif illum <= 8:\n",
    "        m_shape = 'o' # circules\n",
    "    elif illum <= 16:\n",
    "        m_shape = 's' # squares\n",
    "    elif illum <= 32:\n",
    "        m_shape = 'D' # diamonds\n",
    "\n",
    "    # oxygen level encoded by marker fill\n",
    "    if o2 < 20:\n",
    "        m_fill = 'none'\n",
    "    elif o2 < 50:\n",
    "        m_fill = 'right'\n",
    "    else:\n",
    "        m_fill = 'full'\n",
    "\n",
    "    # encapsulation encoded by line style\n",
    "    if encap == 'none':\n",
    "        l_style = ' '\n",
    "    elif encap == '5mgmL_PMMA':\n",
    "        l_style = '--'\n",
    "    elif encap == '10mgmL_PMMA':\n",
    "        l_style = '-'\n",
    "    else:\n",
    "        l_style = ' '\n",
    "\n",
    "    return m_color, m_shape, m_fill, l_style\n",
    "  \n",
    "            \n",
    "# interpolate early time features to a universal time range (default is 10 min)\n",
    "def interp_early_time(time_raw,timeseries,univ_horiz=10):\n",
    "\n",
    "    idx = np.argmin(np.abs(time_raw-univ_horiz)) # index of datapoint closest to the horizon\n",
    "    # if the nth index is at or below the horizon, increment by one;\n",
    "    # otherwise, leave as is so as not to include predictions outside\n",
    "    if time_raw[idx] >= univ_horiz:\n",
    "        idx += 1\n",
    "    else:\n",
    "        idx += 2\n",
    "    # interpolate from range of allowed indices to prediction horizon (5 points after start)\n",
    "    series_interp_func = sp.interpolate.interp1d(time_raw[:idx],timeseries[:idx])\n",
    "    t = np.linspace(0,univ_horiz,num=6)\n",
    "    series_interp = series_interp_func(t)\n",
    "\n",
    "    return(series_interp[-5:])\n",
    "\n",
    "# to make axis box invisible (useful for timeseries plotting with triple axes)\n",
    "# initially written by Ryan Stoddard for the plotplotplot() function\n",
    "def make_patch_spines_invisible(ax):\n",
    "    ax.set_frame_on(True)\n",
    "    ax.patch.set_visible(False)\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_visible(False)\n",
    "\n",
    "# for getting rid of corrupted data and replacing with interpolated approximations\n",
    "def scrub_instrument_malfunction(timeseries,bad_points):\n",
    "    # remove bad points caused by instrument malfunction by interpolating from the adjacent points\n",
    "    # timeseries: array or array-like containing points to interpolate\n",
    "    # bad_points: tuple of tuples: each inner tuple represents the endpoints (inclusive) of each region of bad data\n",
    "    #             in the timeseries if containing two elements, or an isolated bad point if containing only one element\n",
    "    # CAVEAT: only works right now for timeseries with equally spaced datapoints\n",
    "    \n",
    "    # if there are multiple disjoint malfunction regions:\n",
    "    if type(bad_points) == tuple:\n",
    "        # scan over tuples identifying missing points\n",
    "        for bp_idx in bad_points:\n",
    "            # if the point is isolated, recalculate as average of adjacent points\n",
    "            if type(bp_idx) == int:\n",
    "                 timeseries[bp_idx] = 0.5*(timeseries[bp_idx+1] + timeseries[bp_idx-1])\n",
    "            # otherwise, linearly interpolate between the two points adjacent to the range\n",
    "            else:\n",
    "                a = bp_idx[0] # first bad data point\n",
    "                b = bp_idx[1] # \n",
    "                interp_range = b + 2 - a\n",
    "                m_idx = a # missing indices\n",
    "                # while still in the missing index range, keep interpolating points\n",
    "                while m_idx < (b+1):\n",
    "                    timeseries[m_idx] = (m_idx - a + 1)*(timeseries[b+1] - timeseries[a-1])/interp_range + timeseries[a-1]\n",
    "                    m_idx += 1\n",
    "   \n",
    "    # otherwise, for a single isolated point in the entire timeseries:\n",
    "    else:\n",
    "        bp_idx = bad_points\n",
    "        timeseries[bp_idx] = 0.5*(timeseries[bp_idx-1] + timeseries[bp_idx+1])\n",
    "        \n",
    "    # return the scrubbed timeseries\n",
    "    return timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air\n",
      "Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air_3\n"
     ]
    }
   ],
   "source": [
    "# In this cell: load all the data\n",
    "\n",
    "# Paths to directories containing data you wish to analyze\n",
    "#\n",
    "# Format: \n",
    "#\n",
    "# expt_list = ['C:/.../datapath_1',\n",
    "#              'C:/.../datapath_2',\n",
    "#              'et cetera',\n",
    "#             ]  \n",
    "\n",
    "expt_list = ['Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air',\n",
    "             'Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air_3',\n",
    "            ]  \n",
    "\n",
    "    \n",
    "    \n",
    "# Error checking: make sure that the correct experiments are being read in\n",
    "for expt in expt_list:\n",
    "    print(expt)\n",
    "\n",
    "\n",
    "######## ERROR HANDLING: In a limited number of cases, PL traces have some errors (spikes) that make fitting of \n",
    "# PL extinction slightly difficult. Truncating the dataset can help to recover the correct behavior.\n",
    "\n",
    "# If there is a PL spike in the dataset that is higher than the true PL max, \n",
    "# truncate it after the specified number of datapoints\n",
    "high_spike_dict = {'Expt_ID_1':100,\n",
    "                   'Expt_ID_2':50,\n",
    "                  }\n",
    "\n",
    "# If there is a PL spike in the dataset, truncate it after the specified number of datapoints\n",
    "# to avoid problems with PL extinction fitting\n",
    "spike_dict = {'Expt_ID_1':50,\n",
    "              'Expt_ID_2':100,\n",
    "             }\n",
    "\n",
    "# If the PL trace is noisy, this can affect the accuracy of the steepest slope post-PL max fit;\n",
    "# in this case, manually specify the time index to start from\n",
    "too_noisy_dict = {'Expt_ID_1':30,\n",
    "                  'Expt_ID_2':160,\n",
    "                 }\n",
    "\n",
    "\n",
    "# In limited cases (only 1 so far), poor electrical connections or other instrument malfunctions \n",
    "# can cause the signal to drop out in the prediction regime; manually identify these points.\n",
    "# Dictionary values are tuples of tuples; single-value tuples represent isolated points, two-value tuples\n",
    "# represent the endpoints of missing data ranges (inclusive - i.e., (3,7) would indicate that the 3rd through \n",
    "# the 7th - including the 7th - points are missing).\n",
    "dropout_LD_dict = {'Expt_ID_1': (1,(3,4))}\n",
    "dropout_Tr_dict = {'Expt_ID_1': (1)}\n",
    "dropout_PL_dict = {'Expt_ID_1': (2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air\n",
      "['Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air/primary_vids\\\\PL_PC_T_DF_1sun_25C_40RH_air_grad0_loc0_time0']\n",
      "Experiment started at:\n",
      "2021-03-18 10:51:01 -0700\n",
      "Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air_3\n",
      "['Trial_Data/Film_Test_Data/PL_PC_T_DF_1sun_25C_40RH_air_3/primary_vids\\\\PL_PC_T_DF_1sun_25C_40RH_air_3_grad0_loc0_time0']\n",
      "Experiment started at:\n",
      "2021-03-19 12:08:41 -0700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassID</th>\n",
       "      <th>ExptID</th>\n",
       "      <th>Film Thickness [nm]</th>\n",
       "      <th>Temp (deg C)</th>\n",
       "      <th>RH (%)</th>\n",
       "      <th>Oxygen (%)</th>\n",
       "      <th>Illum (Nsuns)</th>\n",
       "      <th>MA fraction</th>\n",
       "      <th>Bleach Rate (polyfit) (1/min)</th>\n",
       "      <th>Bleach Rate (1% inc) (1/min)</th>\n",
       "      <th>...</th>\n",
       "      <th>deg_rate</th>\n",
       "      <th>ddA0</th>\n",
       "      <th>vol_rate</th>\n",
       "      <th>d_vol_rate</th>\n",
       "      <th>initial_PD_power</th>\n",
       "      <th>sigma_ph</th>\n",
       "      <th>Tr_0</th>\n",
       "      <th>Ref_0</th>\n",
       "      <th>tA80</th>\n",
       "      <th>Abs_LD80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210317_Replication_Study</td>\n",
       "      <td>PL_PC_T_DF_1sun_25C_40RH_air</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210317_Replication_Study</td>\n",
       "      <td>PL_PC_T_DF_1sun_25C_40RH_air_3</td>\n",
       "      <td>300</td>\n",
       "      <td>25.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.408233e-08</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-1.121348</td>\n",
       "      <td>-1.114574</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.215</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.257698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ClassID                          ExptID  \\\n",
       "0  210317_Replication_Study    PL_PC_T_DF_1sun_25C_40RH_air   \n",
       "1  210317_Replication_Study  PL_PC_T_DF_1sun_25C_40RH_air_3   \n",
       "\n",
       "   Film Thickness [nm]  Temp (deg C)  RH (%)  Oxygen (%)  Illum (Nsuns)  \\\n",
       "0                  300           0.0     0.0         0.0            0.0   \n",
       "1                  300          25.0    40.0        21.0            1.0   \n",
       "\n",
       "   MA fraction  Bleach Rate (polyfit) (1/min)  Bleach Rate (1% inc) (1/min)  \\\n",
       "0          0.0                       0.000000                           0.0   \n",
       "1          1.0                       0.000328                           NaN   \n",
       "\n",
       "   ...      deg_rate      ddA0  vol_rate  d_vol_rate  initial_PD_power  \\\n",
       "0  ...  0.000000e+00  0.000000  0.000000    0.000000          0.000000   \n",
       "1  ...  5.408233e-08 -0.000033 -1.121348   -1.114574          0.000015   \n",
       "\n",
       "   sigma_ph   Tr_0  Ref_0  tA80  Abs_LD80  \n",
       "0   0.00000  0.000  0.000   0.0  0.000000  \n",
       "1   0.00197  0.069  0.215  70.0  0.257698  \n",
       "\n",
       "[2 rows x 121 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many points to use for early-time fitting\n",
    "points_to_sample = 3 # default is THREE (minimum necessary to fit the 2nd derivative)\n",
    "# standard data acqusition interval, in minutes\n",
    "dt_univ = 5 \n",
    "# standard observation window\n",
    "obs_window = (points_to_sample - 1)*dt_univ\n",
    "\n",
    "# decide whether to use raw or corrected LD for feature extraction\n",
    "use_corrected_LD = False\n",
    "\n",
    "# decide whether to establish a baseline for LD zero point based on PL extinction\n",
    "using_PL_extinction = False\n",
    "\n",
    "# decide whether to use fixed time window\n",
    "use_fixed_time_window = False\n",
    "\n",
    "# initialize list of the experiment names and where to find them\n",
    "ExptIDs = []\n",
    "ClassIDs = []\n",
    "Thicknesses = []\n",
    "\n",
    "\n",
    "# initialize array for factorial analysis for later conversion to dataframe\n",
    "n_runs = len(expt_list)-1\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "# EVERY TIME YOU ADD A NEW FEATURE FOR THE SCRIPT TO CALCULATE, INCREMENT THIS VARIABLE:\n",
    "\n",
    "n_feats_to_extract = 118\n",
    "\n",
    "########################\n",
    "\n",
    "# design matrix: # rows = number of analyzed experiments\n",
    "                 # columns = number of calculated features\n",
    "feat_data = np.zeros([n_runs+1,n_feats_to_extract])\n",
    "\n",
    "\n",
    "# transmittance of a pristine film, in absolute units\n",
    "transmittance_t0 = 0.069\n",
    "# reflectance of a pristine film, in absolute units\n",
    "reflectance_t0 = 0.215\n",
    "\n",
    "# universal sample parameters and physical constants\n",
    "Eg = 1.61 # material band gap, eV\n",
    "photon_flux_1_sun = SQ_calcs.one_sun_photon_flux(Eg)\n",
    "meas_V = 3 # measurement voltage, V\n",
    "q = 1.602e-19 # electron charge, C\n",
    "me = 9.109e-31 # electron mass, kg\n",
    "mc = 0.1*me # electron effective mass, kg\n",
    "mv = 0.1*me # hole effective mass, kg\n",
    "kB = 1.381e-23 # Boltzmann's constant, J/K\n",
    "kB_eV = 8.617e-5 # Boltzmann's constant, J/K\n",
    "h = 6.626e-34 # Planck's constant, J*s\n",
    "\n",
    "rho = 4.16*1e-3/1e-6 # mass density of perovskite, kg/m3\n",
    "mol_mass = 620*1e-3 # molar mass of perovskite, kg/mol\n",
    "alp = 0.961e7 # absorption coefficient, 1/m\n",
    "\n",
    "\n",
    "# MAIN LOOP: iterate over degradation experiments and extract features\n",
    "valid_count = 0\n",
    "for ii,prototype in enumerate(expt_list):\n",
    "        \n",
    "    # make the prototype the first entry in the list of chunks\n",
    "    chunks = []\n",
    "    chunks.append(prototype)\n",
    "\n",
    "    # identify the directory corresponding to each additional chunk, and SORT THEM\n",
    "    chunks += sorted(glob.glob(prototype + '_ctd*'))\n",
    "    \n",
    "    # after loading the prototype data, scan through each chunk in sequence\n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n",
    "        \n",
    "        # By default, assume that the data collection was started fresh each chunk -\n",
    "        # i.e. that it was started either because it was the first chunk, or the whole program timed out\n",
    "        # and Python and MM had to be started together.\n",
    "        MM_crash = False\n",
    "        \n",
    "        # Determine whether the chunk is the prototypical/first portion: certain tasks must be done on this \n",
    "        # iteration (such as getting early-time features) but cannot be done on others, while other tasks are\n",
    "        # unnecessary to repeat (such as loading environment and composition)\n",
    "        if chunk == prototype:\n",
    "            \n",
    "            ##################################\n",
    "            # LOAD ENVIRONMENT AND COMPOSITION\n",
    "            ##################################\n",
    "            \n",
    "            # first, if reading in the first run, grab the environment and sample metadata (only need to do this once)\n",
    "            # extract experiment parameters from the metadata\n",
    "            try:\n",
    "                MDpath = chunk + '/primary_vids/Experiment Info/experiment_info.json'\n",
    "                with open(MDpath) as json_file:\n",
    "                    metadata = json.load(json_file)\n",
    "            except:\n",
    "                MDpath = chunk + '/experiment_info.json'\n",
    "                with open(MDpath) as json_file:\n",
    "                    metadata = json.load(json_file)\n",
    "            \n",
    "            # start by extracting the metadata \n",
    "            ClassIDs.append(metadata['ClassID'])\n",
    "            ExptIDs.append(metadata['ExperimentID'])\n",
    "            T = metadata['Temperature (deg C)']\n",
    "            T_K = T + 273.15 # convert temperature to kelvins\n",
    "            RH = metadata['Atmosphere_RH (%)']\n",
    "            pct_O2 = metadata['Atmosphere_O2 (%)']\n",
    "            N_suns = metadata['Excitation Intensity']\n",
    "            chan_l = metadata['channel_length']\n",
    "            chan_w = metadata['channel_width']\n",
    "            \n",
    "            # figure out whether dark field was taken or not\n",
    "            try: \n",
    "                dark_field = metadata['Dark_Field']\n",
    "            except:\n",
    "                dark_field = False\n",
    "            \n",
    "            # read sample metadata file to get the composition \n",
    "            # (A-site only implemented for now, but should be straightforward to expand)\n",
    "            try:\n",
    "                SDpath = chunk + '/primary_vids/Sample Info/sample_info.json'\n",
    "                with open(SDpath) as json_file:\n",
    "                    sampledata = json.load(json_file)\n",
    "            except:\n",
    "                SDpath = chunk + '/sample_info.json'\n",
    "                with open(SDpath) as json_file:\n",
    "                    sampledata = json.load(json_file)\n",
    "            \n",
    "            # get sample thickness\n",
    "            thickness = sampledata['Film Thickness, nm']\n",
    "            Thicknesses.append(thickness)\n",
    "            \n",
    "            # get A-site composition and parse into a dictionary of component fractions\n",
    "            A_comp = sampledata['Starting Composition A-site'] \n",
    "            parse = A_comp.split(' ')\n",
    "            n = len(parse)\n",
    "            A_comp_dict = {}\n",
    "            for jj in range(int(len(parse)/2)):\n",
    "                A_comp_dict[parse[2*jj]] = parse[2*jj+1]\n",
    "\n",
    "            try:\n",
    "                MA_frac = A_comp_dict['MA']\n",
    "            except:\n",
    "                MA_frac = 0\n",
    "\n",
    "            # try to get stress intensity, if different from probe/excitation intensity\n",
    "            try:\n",
    "                N_suns_stress = metadata['Stress Intensity']\n",
    "            except:\n",
    "                N_suns_stress = N_suns\n",
    "\n",
    "            # try to get encapsulation information\n",
    "            try:\n",
    "                encap = metadata['Encapsulation']\n",
    "            except:\n",
    "                encap = 'none'\n",
    "            \n",
    "            # extract the date fabricated as datetime date object\n",
    "            fab_date = sampledata['Fabrication Date']\n",
    "            mm,dd,yyyy = fab_date.split('/')\n",
    "            fab_date = dt.date(int(yyyy),int(mm),int(dd))\n",
    "            # extract the date measured similarly\n",
    "            meas_date = metadata['Analysis Date']\n",
    "            mm,dd,yyyy = meas_date.split('/')\n",
    "            meas_date = dt.date(int(yyyy),int(mm),int(dd))\n",
    "            # subtract to get storage time\n",
    "            time_in_storage = meas_date - fab_date\n",
    "            days_stored = time_in_storage.days\n",
    "            \n",
    "            # assign timeseries trace colors/markerstyles based on T, RH, NSuns, and O2 level\n",
    "            color, marker, fill, style = envt_plot_style(T,RH,pct_O2,N_suns_stress,encap)\n",
    "            \n",
    "            #########################################\n",
    "            # DETERMINE OVERALL EXPERIMENT START TIME\n",
    "            #########################################\n",
    "            \n",
    "            # determine grand start time of the experiment by looking at the metadata of \n",
    "            # the very first primary video in the entire run\n",
    "            \n",
    "            # get the path to that video\n",
    "\n",
    "            first_vid_path = glob.glob(chunk + '/primary_vids/*time0')\n",
    "            print(first_vid_path)\n",
    "            # open the video metadata file\n",
    "            with open(first_vid_path[0] + '/MMStack_Pos0_metadata.txt') as json_file:\n",
    "                vid_metadata = json.load(json_file)\n",
    "        \n",
    "            \n",
    "            # from the leading entry in the metadata (\"Summary\"), extract the \"StartTime\" string,\n",
    "            # which contains both date and time info\n",
    "            grand_start_time = vid_metadata['Summary']['StartTime']\n",
    "            print('Experiment started at:')\n",
    "            print(grand_start_time)\n",
    "            # convert the string to a \"datetime\" object, which can then be used to determine elapsed times\n",
    "            grand_dt = dt.datetime.strptime(grand_start_time.split(' -')[0], '%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            ####################################################\n",
    "            # LOAD THE FIRST DATA CHUNK AND DO SOME CALCULATIONS\n",
    "            ####################################################\n",
    "            \n",
    "            # extract the timeseries data from the whole run\n",
    "            timeseries_data = pd.read_csv(chunk + '/analyzed_data.csv')\n",
    "            \n",
    "            \n",
    "            # get time, normalized LD, transmittance, and PLQY timeseries\n",
    "            t_min = timeseries_data['t'].values/60\n",
    "            LD = timeseries_data['Low Freq LD [nm]'].values\n",
    "            \n",
    "                \n",
    "            # Scrub out spurious noise datapoints and replace with interpolated values, if applicable\n",
    "            # DO THIS BEFORE THE MOVING AVERAGE\n",
    "            if metadata['ExperimentID'] in dropout_LD_dict:\n",
    "                LD = scrub_instrument_malfunction(LD,dropout_LD_dict[metadata['ExperimentID']])\n",
    "            \n",
    "            # Adjust LD with the moving average, and normalize it to starting value\n",
    "            LD_move_avg = np.zeros(len(t_min)-1)\n",
    "            for jj in range(len(t_min)-1):\n",
    "                LD_move_avg[jj] = (LD[jj+1]+LD[jj])/2\n",
    "            for jj in range(len(LD_move_avg)-1):\n",
    "                LD[jj+1] = (LD_move_avg[jj]+LD_move_avg[jj])/2\n",
    "            \n",
    "            # replace the LD data with the smoothed data\n",
    "            timeseries_data['Low Freq LD [nm]'] = LD\n",
    "                            \n",
    "            # set the timeseries dataframe of the prototype run as the base dataset to which all future data will be added\n",
    "            grand_DF = timeseries_data\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            #######################################\n",
    "            # DETERMINE EXPERIMENT CHUNK START TIME\n",
    "            #######################################\n",
    "            \n",
    "            # determine start time of the chunk from the first primary video in that chunk\n",
    "            first_vid_path = glob.glob(chunk + '/primary_vids/*time0')\n",
    "            print(first_vid_path)\n",
    "            # open the video metadata file\n",
    "            with open(first_vid_path[0] + '/MMStack_Pos0_metadata.txt') as json_file:\n",
    "                vid_metadata = json.load(json_file)\n",
    "            # extract the start time of the chunk\n",
    "            chunk_start_time = vid_metadata['Summary']['StartTime']\n",
    "            print('Next chunk started at:')\n",
    "            print(chunk_start_time)\n",
    "            # convert to datetime object, and calculate elapsed time since start of run\n",
    "            chunk_dt = dt.datetime.strptime(chunk_start_time.split(' -')[0], '%Y-%m-%d %H:%M:%S')\n",
    "            time_delta = chunk_dt - grand_dt # time_delta object\n",
    "            print('Total time elapsed since start of run (min):')\n",
    "            elapsed_min = round(time_delta.total_seconds()/60,0) # floating point number rounded to nearest minute\n",
    "            print(elapsed_min)\n",
    "            \n",
    "            ##############################\n",
    "            # DETERMINE WHAT WAS RESTARTED\n",
    "            ##############################\n",
    "            \n",
    "            # determine whether MicroManager crashed or the whole DAQ system (Python + MM) was restarted\n",
    "            # open experiment info metadata\n",
    "            MDpath = chunk + '/primary_vids/Experiment Info/experiment_info.json'\n",
    "            with open(MDpath) as json_file:\n",
    "                metadata = json.load(json_file)\n",
    "            MM_crash = (metadata['Ld_data'] == False) or (metadata['Transmissivity_data'] == False)\n",
    "            print('MicroManager Only Crashed? ',MM_crash)\n",
    "            \n",
    "            #####################\n",
    "            #FILL IN MISSING DATA\n",
    "            #####################\n",
    "            \n",
    "            # extract the timeseries data for the chunk\n",
    "            timeseries_data = pd.read_csv(chunk + '/analyzed_data.csv')\n",
    "\n",
    "            \n",
    "            # if the whole run was reset (both Python and MM restarted), simply append the entire timeseries DF\n",
    "            # to the grand DF, after smoothing LD and adjusting the time\n",
    "            if not MM_crash:\n",
    "                # Adjust LD with the moving average, and normalize it to starting value\n",
    "                t_min = timeseries_data['t'].values/60\n",
    "                LD = timeseries_data['Low Freq LD [nm]'].values\n",
    "                LD_move_avg = np.zeros(len(t_min)-1)\n",
    "                for jj in range(len(t_min)-1):\n",
    "                    LD_move_avg[jj] = (LD[jj+1]+LD[jj])/2\n",
    "                for jj in range(len(LD_move_avg)-1):\n",
    "                    LD[jj+1] = (LD_move_avg[jj+1]+LD_move_avg[jj])/2\n",
    "                LD_norm = LD/LD[0]\n",
    "                # replace the LD data with the smoothed data\n",
    "                timeseries_data['Low Freq LD [nm]'] = LD\n",
    "                \n",
    "                # correct the time vector values based on the time elapsed since the start\n",
    "                timeseries_data['t'] += elapsed_min*60\n",
    "                \n",
    "                # append the corrected dataframe to the grand DF\n",
    "                grand_DF = grand_DF.append(timeseries_data,ignore_index=True)\n",
    "                \n",
    "            \n",
    "            # if only MicroManager crashed and was restarted, insert the timeseries into grand_DF in the appropriate place\n",
    "            else:\n",
    "                # find the appropriate spot to insert the data\n",
    "                idx_to_insert = np.abs(grand_DF['t']-elapsed_min*60).idxmin()\n",
    "                # calculate how many spots are left in grand_DF \n",
    "                # (so we don't add PL data after the Python script stopped collecting) \n",
    "                idxs_left=grand_DF.shape[0]-idx_to_insert\n",
    "                print(grand_DF['t'][idx_to_insert]/60)\n",
    "                \n",
    "                # correct the time vector values based on the time elapsed since the start\n",
    "                timeseries_data['t'] += elapsed_min*60\n",
    "                \n",
    "                # insert the data in the correct spot in grand_DF\n",
    "                for col in timeseries_data.columns:\n",
    "                    if col != 't':\n",
    "                        try:\n",
    "                            grand_DF[col][idx_to_insert:idx_to_insert+len(timeseries_data[col][:idxs_left])] = timeseries_data[col][:idxs_left]\n",
    "                        except KeyError:\n",
    "                            print('Tried to add field not in grand_DF')\n",
    "                            continue\n",
    "    \n",
    "    #########################################################################################\n",
    "    # Now the full timeseries dataset has been reconstructed and we can do feature extraction \n",
    "    #########################################################################################\n",
    "        \n",
    "    # calculate time, in minutes, elapsed since start of the experiment\n",
    "    t_min = grand_DF['t'].values/60.0\n",
    "    \n",
    "    # data acquisition interval for this run (may be different from the standard run)\n",
    "    dt_this = t_min[1] - t_min[0]\n",
    "    \n",
    "    # how many points we get to look at to make predictions\n",
    "    if use_fixed_time_window:\n",
    "        n_look = math.floor(obs_window/dt_this) + 1\n",
    "    else:\n",
    "        n_look = points_to_sample\n",
    "    \n",
    "    # Get full transmittance trace\n",
    "    photodiode_t0 = grand_DF['Transmitted Power [W]'].values[0] # initial photodiode measurement\n",
    "    \n",
    "    # convert normalized to absolute transmittance...\n",
    "    Tr_absolute = transmittance_t0*grand_DF['Transmitted Power [W]'].values/photodiode_t0\n",
    "    \n",
    "    # ...and calculate light absorption traces\n",
    "    absorptance = 1 - Tr_absolute - reflectance_t0 # FRACTION of incident light absorbed\n",
    "    Absorbance = -np.log10(Tr_absolute/(1-reflectance_t0)) # LOGARITHM of film's internal transmittance\n",
    "    Abs_norm = Absorbance/(Absorbance[0])\n",
    "    \n",
    "    # Scrub out spurious noise datapoints and replace with interpolated values, if applicable\n",
    "    if metadata['ExperimentID'] in dropout_Tr_dict:\n",
    "        Tr_absolute = scrub_instrument_malfunction(Tr_absolute,dropout_Tr_dict[metadata['ExperimentID']]) \n",
    "\n",
    "    # calculate initial photobleaching rate from normalized transmittance using several different methods\n",
    "            \n",
    "    # polynomial fit first\n",
    "    pTr = np.polyfit(t_min[:n_look],Tr_absolute[:n_look],1)\n",
    "    pBleach_rate = pTr[0] # slope\n",
    "    pCoeff = pTr[1] # intercept\n",
    "    bleach_rate_pfit = pBleach_rate\n",
    "\n",
    "    # calculate initial transmittance concavity from norm. transmittance\n",
    "    pTr = np.polyfit(t_min[:n_look],Tr_absolute[:n_look],2)\n",
    "    Tr_concavity = pTr[2] # quadratic coefficient\n",
    "\n",
    "    # calculate bleaching rate from time to reach x%\n",
    "    bleach_rate_1_pct = bleach_rate_from_pct_increase(1.01,t_min,Tr_absolute)\n",
    "    bleach_rate_2_pct = bleach_rate_from_pct_increase(1.02,t_min,Tr_absolute)\n",
    "    bleach_rate_5_pct = bleach_rate_from_pct_increase(1.05,t_min,Tr_absolute)\n",
    "\n",
    "    # calculate bleaching rate from LINEAR FIT to time to reach x%\n",
    "    bleach_rate_fit_1_pct, intercept_1pct = bleach_rate_from_linear_fit_to_pct_increase(1.01,t_min,Tr_absolute)\n",
    "    bleach_rate_fit_2_pct, intercept_2pct = bleach_rate_from_linear_fit_to_pct_increase(1.02,t_min,Tr_absolute)\n",
    "    bleach_rate_fit_5_pct, intercept_5pct = bleach_rate_from_linear_fit_to_pct_increase(1.05,t_min,Tr_absolute)\n",
    "        \n",
    "    # Calculate timeseries derived from transmittance - absorbance, absorptance, material decomposition rate\n",
    "    Tr_norm = Tr_absolute/Tr_absolute[0] # renormalize transmittance\n",
    "    delta_A = -np.log10(Tr_norm) # calculate change in absorbance\n",
    "    Abs_fit = np.polyfit(t_min[:n_look],delta_A[:n_look],1) # perform linear fit to initial data\n",
    "    dAdt = Abs_fit[0]/60 # fit the 1st time derivative of absorbance\n",
    "    vol_rate = dAdt/(thickness*1e-7) # calculate volumetric degradation rate (cm^-1 min^-1)\n",
    "    \n",
    "    # calculate the degradation rate\n",
    "    deg_rate = -rho/(mol_mass*alp*np.log10(math.e))*dAdt # [mol/m2/s]\n",
    "    \n",
    "    # Extract LD-related features: LD_0 and t_LD_75 and t_LD_80\n",
    "    LD = grand_DF['Low Freq LD [nm]'].values\n",
    "    # interpolate it to the new time grid\n",
    "    \n",
    "    # normalize raw LD\n",
    "    LD_norm = LD/LD[0]\n",
    "        \n",
    "    # Extract PL-related features\n",
    "    PLQY = grand_DF['PLQY_xy0t0'].values/absorptance\n",
    "    xy1t0 = grand_DF['xy1t0'].values/absorptance # spatial standard deviation of time average\n",
    "    xy0t1 = grand_DF['xy0t1'].values/absorptance # spatial average of time standard deviation\n",
    "    xy0t1Norm = grand_DF['xy0t1Norm'].values # spatial average of time standard deviation, normalized by mean\n",
    "    xy1t1 = grand_DF['xy1t1'].values/absorptance # spatial standard deviation of time standard deviation\n",
    "    t0xy1 = grand_DF['t0xy1'].values/absorptance # time mean of spatial s.d.\n",
    "    t1xy0 = grand_DF['t1xy0'].values/absorptance # time s.d. of spatial mean\n",
    "    frac_bright = grand_DF['frac_bright'].values # fraction of photobrightening pixels\n",
    "    QFLS = grand_DF['QFLS_xy0t0'].values # quasi Fermi level splitting\n",
    "    xy2t0 = grand_DF['xy2t0'].values/absorptance # spatial skewness of time average\n",
    "    xy3t0 = grand_DF['xy3t0'].values/absorptance # spatial kurtosis of time average\n",
    "    xy2t1 = grand_DF['xy2t1'].values/absorptance # spatial skewness of time s.d.\n",
    "    xy3t1 = grand_DF['xy3t1'].values/absorptance # spatial kurtosis of time s.d.\n",
    "    beta_mean = grand_DF['beta_mean_xy_vs_t'].values/absorptance # mean photobrightening rate\n",
    "    beta_std = grand_DF['beta_std_xy_vs_t'].values/absorptance # mean photobrightening rate\n",
    "    cv_slopes = grand_DF['cv_slopes'].values # slope of the video coefficient of variation\n",
    "    \n",
    "    # Scrub out spurious noise datapoints and replace with interpolated values, if applicable\n",
    "    if metadata['ExperimentID'] in dropout_PL_dict:\n",
    "        PLQY = scrub_instrument_malfunction(PLQY,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy1t0 = scrub_instrument_malfunction(xy1t0,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy0t1 = scrub_instrument_malfunction(xy0t1,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy1t1 = scrub_instrument_malfunction(xy1t1,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        t0xy1 = scrub_instrument_malfunction(t0xy1,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        t1xy0 = scrub_instrument_malfunction(t1xy0,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        QFLS = scrub_instrument_malfunction(QFLS,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy2t0 = scrub_instrument_malfunction(xy2t0,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy3t0 = scrub_instrument_malfunction(xy3t0,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        xy2t1 = scrub_instrument_malfunction(xy3t1,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        beta_mean = scrub_instrument_malfunction(beta_mean,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        beta_std = scrub_instrument_malfunction(beta_std,dropout_PL_dict[metadata['ExperimentID']])\n",
    "        cv_slopes = scrub_instrument_malfunction(cv_slopes,dropout_PL_dict[metadata['ExperimentID']])\n",
    "    \n",
    "    \n",
    "    # replace zeros with NaNs so we can take the natural log\n",
    "    PLQY[np.where(PLQY==0)] = np.nan\n",
    "    \n",
    "    # Determine time at which PL reaches a maximum:\n",
    "    \n",
    "    # if max PL happens at the end of the experiment, do not assign PL max\n",
    "    if np.nanmax(PLQY) == np.array(PLQY)[-1]:\n",
    "        PLmax_exists = False\n",
    "        t_PLmax = np.nan\n",
    "    else:\n",
    "        PLmax_exists = True\n",
    "        # if there is a spurious PL max, avoid the time range containing the spike\n",
    "        if metadata['ExperimentID'] in high_spike_dict.keys():\n",
    "            t_PLmax_idx = np.nanargmax(PLQY[:high_spike_dict[metadata['ExperimentID']]])\n",
    "        else:    \n",
    "            t_PLmax_idx = np.nanargmax(PLQY)\n",
    "        t_PLmax = t_min[t_PLmax_idx]\n",
    "        PLmax = PLQY[t_PLmax_idx]\n",
    "        \n",
    "        # If PLmax happens when MicroManager crashes, then don't count it\n",
    "        if np.isnan(PLQY[t_PLmax_idx+1]):\n",
    "            PLmax_exists = False\n",
    "            t_PLmax = np.nan\n",
    "            PLmax = np.nan\n",
    "\n",
    "    \n",
    "    # first determine whether normalized LD trace passes through 75% or not\n",
    "    LD75_exists = False\n",
    "    for jj in range(len(LD_norm)):\n",
    "        if (0.75-LD_norm[jj]) > 0:\n",
    "            LD75_exists = True\n",
    "            LD75_idx = jj\n",
    "            break        \n",
    "    # if LD failure is witnessed, identify time at failure\n",
    "    if LD75_exists:\n",
    "        tLD75 = t_min[LD75_idx]\n",
    "    else:\n",
    "        tLD75 = np.nan\n",
    "    \n",
    "    # first determine whether normalized LD trace passes through 80% or not\n",
    "    LD80_exists = False\n",
    "    # filter through to find the first point at which LD crosses 80% of starting value\n",
    "    for jj in range(len(LD_norm)):\n",
    "        if (0.8-LD_norm[jj]) > 0:\n",
    "            LD80_exists = True\n",
    "            LD80_idx = jj\n",
    "            break        \n",
    "    # if LD failure is witnessed, identify time at failure\n",
    "    if LD80_exists:\n",
    "        tLD80 = t_min[LD80_idx]\n",
    "    else:\n",
    "        tLD80 = np.nan\n",
    "        \n",
    "    # do the same for absorbance\n",
    "    A80_exists = False\n",
    "    # filter through to find the first point at which LD crosses 80% of starting value\n",
    "    for jj in range(len(Abs_norm)):\n",
    "        if (0.8-Abs_norm[jj]) > 0:\n",
    "            A80_exists = True\n",
    "            A80_idx = jj\n",
    "            break        \n",
    "    # if LD failure is witnessed, identify time at failure\n",
    "    if A80_exists:\n",
    "        tA80 = t_min[A80_idx]\n",
    "    else:\n",
    "        tA80 = np.nan\n",
    "    \n",
    "    ##### CORRECT LD FOR ABSORPTANCE AND PHOTOCURRENT BACKGROUND\n",
    "    \n",
    "    # The LD calculated from the raw photoconductivity signal may be inaccurate for two reasons:\n",
    "    # 1. Light absorption not being taken into account (leads to systematic underestimate)\n",
    "    # 2. Contribution to photocurrent from beam edges, which degrade slower than the sample under the direct beam - \n",
    "    # small background currents can have a large impact on the calculated diffusion length because of the square root \n",
    "    # dependence (leads to systematic overestimate) \n",
    "    #\n",
    "    # In practice, these corrections mostly cancel each other out, so the uncorrected LD is usually a fairly accurate \n",
    "    # metric over the time range of interest (t=0 to tLD,75). Methods for \n",
    "    #\n",
    "    # Absorption is fairly easy to quantify if the initial transmittance and reflectance are known. Background current is \n",
    "    # harder, but we provide two methods for identifying it. The first, most general method, is to identify \n",
    "    # the point of steepest decline in the LD curve and then take the first point afterward at which LD is observed to \n",
    "    # increase (rising LD in this case will generally signify that the background is undergoing its own passivation event \n",
    "    # on a much slower timescale, or that LD has flattened out enough for measurement noise to lead to a positive first \n",
    "    # derivative). The photocurrent here is then taken and subtracted from the original photocurrent timeseries. If a positive \n",
    "    # first derivative is never observed, then the background photocurrent is taken at the final point in the timeseries.\n",
    "    # A more sophisticated way to determine this time is to use the PL extinction time: when the observable photoluminescence \n",
    "    # has been quenched in the imaged region, it is safe to assume that any photoconductivity signal comes from the external \n",
    "    # region. The \"PL extinction\" time is determined by finding the point of steepest PL decline after the maximum, taking a\n",
    "    # linear fit, and calculating the x-intercept of the fit. (PL decay usually has a long tail due to the slow decay of \n",
    "    # a few very bright but noncontiguous domains.)\n",
    "\n",
    "    \n",
    "    # Background Current Estimation Method #1: time of first increase in LD after steepest decline\n",
    "    \n",
    "    # calculate the time derivative of the diffusion length and PLQY\n",
    "    dLDdt = []\n",
    "    for jj in range(len(LD) - 1):\n",
    "        dLDdt.append((LD[jj+1]-LD[jj])/(t_min[jj+1]-t_min[jj]))\n",
    "    # convert to an array   \n",
    "    dLDdt = np.array(dLDdt)\n",
    "    jj = np.nanargmin(dLDdt[:-1]) # find the index corresponding to the time at which LD decays most steeply\n",
    "    # after this time point, iterate until LD starts increasing again\n",
    "    while dLDdt[jj] < 0:\n",
    "        jj += 1\n",
    "        # if the last time point in the series was reached without increasing LD, stop\n",
    "        # and use that point\n",
    "        if jj == len(dLDdt):\n",
    "            zero_idx = -1\n",
    "            break\n",
    "    # assign the zero index\n",
    "    zero_idx = jj\n",
    "    \n",
    "    # Background Current Estimation Method #2: use PL extinction\n",
    "    \n",
    "    if using_PL_extinction:\n",
    "            \n",
    "        # OR, determine the PL extinction time mathematically as the x-intercept of the down slope of the PL curve\n",
    "        if PLmax_exists:\n",
    "            \n",
    "            # if the PL max is obscured by noise, manually redefine starting time to search for steepest PL slope\n",
    "            if metadata['ExperimentID'] in too_noisy_dict.keys():\n",
    "                t_start = t_min[too_noisy_dict[metadata['ExperimentID']]]\n",
    "            else:\n",
    "                t_start = t_PLmax\n",
    "            \n",
    "            # if there's a spike in PL that causes a huge spurious minimum in dPLQY/dt, \n",
    "            # constrain the search range\n",
    "            if metadata['ExperimentID'] in spike_dict.keys():\n",
    "                range_end = spike_dict[metadata['ExperimentID']]\n",
    "            else:\n",
    "                range_end = -1\n",
    "                \n",
    "            PLQY_dying = PLQY[np.where(t_min[:range_end] > t_start)] # get only the portion of the PL curve after PL max\n",
    "            t_PLQY_dying = t_min[np.where(t_min[:range_end] > t_start)] # corresponding time range\n",
    "            dPLQY_dying_dt = np.gradient(PLQY_dying,t_PLQY_dying) # take the time derivative of the PL in the dying regime\n",
    "            m = np.min(dPLQY_dying_dt) # steepest slope # calculate the PL slope at the point of steepest decline\n",
    "            b = PLQY_dying[np.argmin(dPLQY_dying_dt)] - m*t_PLQY_dying[np.argmin(dPLQY_dying_dt)] # y-intercept of linear fit\n",
    "            t_extinction = -b/m # extinction time when the linear fit crosses the x-axis - i.e., PL = 0\n",
    "            alt_zero_idx = np.argmin(np.abs(t_min-t_extinction))           \n",
    "        else:\n",
    "            alt_zero_idx = np.nan\n",
    "        alt_zero_idxs.append(alt_zero_idx)\n",
    "        \n",
    "        if ~np.isnan(alt_zero_idx):\n",
    "            zero_idx = alt_zero_idx\n",
    "            \n",
    "        \n",
    "        \n",
    "    # reconstructed photocurrent, amps\n",
    "    I_ph = (LD*1e-9)**2*(2*q**2*meas_V*N_suns*photon_flux_1_sun*chan_w)/(chan_l*kB*T_K)\n",
    "       \n",
    "    # subtract off photocurrent remaining at final LD trace flattening\n",
    "    I_ph_bkg_cor = I_ph - I_ph[zero_idx]\n",
    "    \n",
    "    # set background corrected photocurrent to zero where it appears to be less than the background\n",
    "    I_ph_bkg_cor[np.where(I_ph_bkg_cor < 0)] = 0\n",
    "    \n",
    "\n",
    "    # correct LD metrics for absorptance changes and background effects\n",
    "    LD_bkg_cor = 1e9*np.sqrt((I_ph_bkg_cor*chan_l*kB*T_K)/(2*q**2*meas_V*N_suns*photon_flux_1_sun*chan_w))\n",
    "    LD_full_cor = LD_bkg_cor/np.sqrt(absorptance)\n",
    "    LD_abs_cor = LD/np.sqrt(absorptance)\n",
    "    LD_full_cor_norm = LD_full_cor/LD_full_cor[0]\n",
    "    \n",
    "    \n",
    "    abs_depth = 210 # absorption depth, nm\n",
    "    # effective channel depth is either absorption depth plus diffusion length OR film thickness, whichever is less\n",
    "    channel_depth = np.min([thickness, abs_depth + LD_full_cor[0]])\n",
    "    \n",
    "    # reconstructed photoconductivity\n",
    "    sigma_ph_0 = (I_ph_bkg_cor[0]/meas_V)*(chan_l/(chan_w*channel_depth*1e-9))\n",
    "    \n",
    "        \n",
    "    # identify if LD75 exists and what it is for corrected LD\n",
    "    LD75_cor_exists = False\n",
    "    for jj in range(len(LD_full_cor_norm)):\n",
    "        if (0.75-LD_full_cor_norm[jj]) > 0:\n",
    "            LD75_cor_exists = True\n",
    "            LD75_cor_idx = jj\n",
    "            break        \n",
    "    # if LD failure is witnessed, identify time at failure\n",
    "    if LD75_cor_exists:\n",
    "        tLD75_cor = t_min[LD75_cor_idx-1] + (t_min[LD75_cor_idx]-t_min[LD75_cor_idx-1])/((LD_full_cor_norm[LD75_cor_idx]-LD_full_cor_norm[LD75_cor_idx-1]))*(0.75-LD_full_cor_norm[LD75_cor_idx-1])\n",
    "    else:\n",
    "        tLD75_cor = np.nan\n",
    "    \n",
    "    # do the same for LD80\n",
    "    LD80_cor_exists = False\n",
    "    # identify the index at which LD falls below 80%\n",
    "    for jj in range(len(LD_full_cor_norm)):\n",
    "        if (0.80-LD_full_cor_norm[jj]) > 0:\n",
    "            LD80_cor_exists = True\n",
    "            LD80_cor_idx = jj\n",
    "            break        \n",
    "    # if LD failure is witnessed, identify time at failure\n",
    "    if LD80_cor_exists:\n",
    "        tLD80_cor = t_min[LD80_cor_idx-1] + (t_min[LD80_cor_idx]-t_min[LD80_cor_idx-1])/((LD_full_cor_norm[LD80_cor_idx]-LD_full_cor_norm[LD80_cor_idx-1]))*(0.80-LD_full_cor_norm[LD80_cor_idx-1])\n",
    "    else:\n",
    "        tLD80_cor = np.nan\n",
    "    \n",
    "    # correct initial LD\n",
    "    LD0_cor = LD_full_cor[0]\n",
    "    \n",
    "    PL_norm = PLQY/PLQY[0] # recalculate normalized PL\n",
    "    \n",
    "    # calculate carrier lifetime and mobility\n",
    "    photon_flux = N_suns*SQ_calcs.one_sun_photon_flux(Eg)\n",
    "    Nc = 2*(2*np.pi*mc*kB*T_K/h**2)**(3/2) # CB effective DOS, 1/m3\n",
    "    Nv = 2*(2*np.pi*mv*kB*T_K/h**2)**(3/2) # VB effective DOS, 1/m3\n",
    "    ni = (Nc*Nv*np.exp(-Eg/(kB_eV*T_K)))**0.5 # intrinsic carrier cxn, 1/m3\n",
    "    n_exc = ni*(np.exp(QFLS/(kB_eV*T_K)))**0.5 # excited carrier cxn, 1/m3\n",
    "    tau = 1e9*thickness*1e-9/(2*photon_flux)*n_exc*absorptance**-1.5 # carrier lifetime, ns\n",
    "    sigma_ph = 2*(q**2)*photon_flux*(LD_bkg_cor*1e-9)**2/(thickness*1e-9*kB*T_K) # re-extract the photoconductivity\n",
    "    mu = sigma_ph/(2*q*n_exc)*absorptance**0.5 # carrier mobility, m2/V*s\n",
    "    \n",
    "    # calculate carrier lifetime and mobility at LD80 (corrected)\n",
    "    if LD80_cor_exists and grand_DF['frame_corrupted'].iloc[LD80_cor_idx]==False:\n",
    "        tau_LD80 = tau[LD80_cor_idx]/tau[0]\n",
    "        mu_LD80 = mu[LD80_cor_idx]/mu[0]\n",
    "    else:\n",
    "        tau_LD80 = np.nan\n",
    "        mu_LD80 = np.nan\n",
    "    \n",
    "    # calculate carrier lifetime and mobility at LD75 (corrected)\n",
    "    if LD75_cor_exists and grand_DF['frame_corrupted'].iloc[LD75_cor_idx]==False:\n",
    "        tau_LD75 = tau[LD75_cor_idx]/tau[0]\n",
    "        mu_LD75 = mu[LD75_cor_idx]/mu[0]\n",
    "    else:\n",
    "        tau_LD75 = np.nan\n",
    "        mu_LD75 = np.nan\n",
    "    \n",
    "    tau_norm = tau/tau[0]\n",
    "    mu_norm = mu/mu[0]\n",
    "    \n",
    "    # extract fits of tau and mu trends\n",
    "    try:\n",
    "        tau_fit = np.polyfit(t_min[:n_look],tau_norm[:n_look],1)\n",
    "        dtaudt = tau_fit[0]\n",
    "        # handle issue where corrupt PL videos in modeling period cause mu to explode from divide by zero error\n",
    "        corrupt = grand_DF['frame_corrupted'].values[:n_look]\n",
    "        mu_fit = np.polyfit(t_min[np.where(~corrupt)[0]],mu_norm[np.where(~corrupt)[0]],1)\n",
    "        dmudt = mu_fit[0]\n",
    "    except:\n",
    "        tau_fit = np.polyfit(t_min[:3],tau_norm[:3],1)\n",
    "        dtaudt = tau_fit[0]\n",
    "        try:\n",
    "            mu_fit = np.polyfit(t_min[:3],mu_norm[:3],1)\n",
    "            dmudt = mu_fit[0]\n",
    "        except:\n",
    "            dmudt = np.nan\n",
    "    \n",
    "\n",
    "    \n",
    "    # find LD max\n",
    "    if use_corrected_LD:\n",
    "        LD_trace_to_use = LD_full_cor_norm\n",
    "    else:\n",
    "        LD_trace_to_use = LD_norm\n",
    "        \n",
    "    if np.nanmax(LD_trace_to_use) == np.array(LD_trace_to_use)[-1]:\n",
    "        LDmax_exists = False\n",
    "        t_LDmax = np.nan\n",
    "    else:\n",
    "        LDmax_exists = True\n",
    "        t_LDmax_idx = np.nanargmax(LD_trace_to_use)\n",
    "        t_LDmax = t_min[t_LDmax_idx]\n",
    "        LDmax = LD_full_cor_norm[t_LDmax_idx]\n",
    "        \n",
    "        # If PLmax happens when MM crashes, then don't count it\n",
    "        if np.isnan(LD_trace_to_use[t_LDmax_idx+1]):\n",
    "            LDmax_exists = False\n",
    "            t_LDmax = np.nan\n",
    "            LDmax = np.nan\n",
    "            \n",
    "    \n",
    "    # try to find point at which PL hits 10% of max\n",
    "    t_PLmax_10 = np.nan\n",
    "    if PLmax_exists:\n",
    "        for jj, PL in enumerate(PLQY[t_PLmax_idx:]):\n",
    "            if (0.1-PL/PLmax) > 0:\n",
    "                t_PLmax_10 = t_min[jj+t_PLmax_idx]\n",
    "                break\n",
    "                \n",
    "    # try to find point at which PL hits 10% of starting value\n",
    "    t_PL10 = np.nan\n",
    "    for jj in range(len(PLQY)):\n",
    "        if (0.1 - PLQY[jj]/PLQY[0]) > 0:\n",
    "            t_PL10 = t_min[jj]\n",
    "            break\n",
    "            \n",
    "    # fit initial LD slope\n",
    "    if use_corrected_LD:\n",
    "        polyLD = np.polyfit(t_min[:n_look],LD_full_cor_norm[:n_look],1)\n",
    "    else:\n",
    "        polyLD = np.polyfit(t_min[:n_look],LD_norm[:n_look],1)\n",
    "    dLDdt_0 = polyLD[0]\n",
    "\n",
    "    # fit initial PL slope\n",
    "    try:\n",
    "        polyPL = np.polyfit(t_min[:n_look],PL_norm[:n_look],1)\n",
    "    except:    \n",
    "        polyPL = np.polyfit(t_min[:3],PL_norm[:3],1)\n",
    "    dPLdt_0 = polyPL[0]\n",
    "            \n",
    "            \n",
    "    # get initial transmittance concavity\n",
    "    ddTr0 = np.mean(np.gradient(np.gradient(Tr_absolute[:n_look],t_min[:n_look]),\\\n",
    "                                t_min[:n_look]))\n",
    "    # get initial absorbance concavity\n",
    "    ddA0 = np.mean(np.gradient(np.gradient(delta_A[:n_look],t_min[:n_look]),\\\n",
    "                                t_min[:n_look]))\n",
    "    # derivative of volumetric decomposition rate\n",
    "    d_vol_rate = ddA0/(thickness*1e-7)\n",
    "    \n",
    "    # get initial LD concavity\n",
    "    if use_corrected_LD:\n",
    "        ddLD0 = np.mean(np.gradient(np.gradient(LD_full_cor_norm[:n_look],t_min[:n_look]),t_min[:n_look])) \n",
    "    else:            \n",
    "        ddLD0 = np.mean(np.gradient(np.gradient(LD_norm[:n_look],t_min[:n_look]),t_min[:n_look]))  \n",
    "                                  \n",
    "    # get initial PL, mu, and tau concavity\n",
    "    try:\n",
    "        ddPL0 = np.mean(np.gradient(np.gradient(PL_norm[:n_look],t_min[:n_look]),\\\n",
    "                                t_min[:n_look]))\n",
    "        # again handle cases where PL videos are corrupt\n",
    "        corrupt = grand_DF['frame_corrupted'].values[:n_look]\n",
    "        ddmu0 = np.nanmean(np.gradient(np.gradient(mu_norm[np.where(~corrupt)[0]],t_min[np.where(~corrupt)[0]]),\\\n",
    "                                t_min[np.where(~corrupt)[0]]))        \n",
    "        ddtau0 = np.mean(np.gradient(np.gradient(tau_norm[:n_look][~mu_nans],t_min[:n_look]),\\\n",
    "                                t_min[:n_look]))\n",
    "    except:\n",
    "        ddPL0 = np.mean(np.gradient(np.gradient(PL_norm[:3],t_min[:3]),\\\n",
    "                                t_min[:3]))    \n",
    "        # again handle cases where PL videos are corrupt\n",
    "        corrupt = grand_DF['frame_corrupted'].values[:3]\n",
    "        ddmu0 = np.nanmean(np.gradient(np.gradient(mu_norm[np.where(~corrupt)[0]],t_min[np.where(~corrupt)[0]]),\\\n",
    "                                t_min[np.where(~corrupt)[0]]))        \n",
    "        ddtau0 = np.mean(np.gradient(np.gradient(tau_norm[:3],t_min[:3]),\\\n",
    "                                t_min[:3]))\n",
    "    \n",
    "    # get LD, PL, and Tr at first five time points after start\n",
    "    n_use = 3 # number of points to use\n",
    "    LD_early = LD_full_cor_norm[1:n_use+1]\n",
    "    Tr_early = Tr_absolute[1:n_use+1]\n",
    "    PL_early = PL_norm[1:n_use+1]\n",
    "    mu_early = mu_norm[1:n_use+1]\n",
    "    tau_early = tau_norm[1:n_use+1]\n",
    "    \n",
    "    \n",
    "    # how much time elapses during initial prediction interval (so we know to throw out runs that fail during this period)\n",
    "    pred_horiz = t_min[n_use+1]\n",
    "    \n",
    "    # Predictions based on pre-specified time range\n",
    "    LD_1st_10 = interp_early_time(t_min,LD_full_cor_norm,10)\n",
    "    PL_1st_10 = interp_early_time(t_min,PL_norm,10)\n",
    "    Tr_1st_10 = interp_early_time(t_min,Tr_absolute,10)                              \n",
    "    \n",
    "    # scaling transmittance curves to absolute optical units\n",
    "    # look at ending slope; if y(Tr)-intercept is over 90% of the starting value, use it for calculation of Tr_max\n",
    "    tr_end = Tr_absolute[-1] # norm. photodiode signal at end of experiment\n",
    "    t_end = t_min[-1] # time at end of experiment\n",
    "    try:\n",
    "        end_slope = np.polyfit(t_min[-10:],Tr_absolute[-10:],1)[0] # photodiode signal slope at the end of the experiment\n",
    "        if end_slope*t_end < 0.33*tr_end:\n",
    "            Tr_finished = True\n",
    "        else:\n",
    "            Tr_finished = False\n",
    "    except:\n",
    "        Tr_finished = False\n",
    "        \n",
    "    # calculate dark-field related features\n",
    "    if dark_field:\n",
    "        DF_mean = grand_DF['DF_means']\n",
    "        DF_median = grand_DF['DF_medians']\n",
    "        DF_std = grand_DF['DF_stds']\n",
    "        DF_skew = grand_DF['DF_skews']\n",
    "        DF_kurt = grand_DF['DF_kurts']\n",
    "\n",
    "        ddDFmean0 = np.mean(np.gradient(np.gradient(DF_mean[:n_look],t_min[:n_look]),\\\n",
    "                                    t_min[:n_look]))\n",
    "        ddDFmedian0 = np.mean(np.gradient(np.gradient(DF_median[:n_look],t_min[:n_look]),\\\n",
    "                                    t_min[:n_look]))\n",
    "        ddDFstd0 = np.mean(np.gradient(np.gradient(DF_std[:n_look],t_min[:n_look]),\\\n",
    "                                    t_min[:n_look]))\n",
    "        ddDFskew0 = np.mean(np.gradient(np.gradient(DF_skew[:n_look],t_min[:n_look]),\\\n",
    "                                    t_min[:n_look]))    \n",
    "        ddDFkurt0 = np.mean(np.gradient(np.gradient(DF_kurt[:n_look],t_min[:n_look]),\\\n",
    "                                    t_min[:n_look]))    \n",
    "\n",
    "        poly_DFmean = np.polyfit(t_min[:n_look],DF_mean[:n_look],1)\n",
    "        poly_DFmedian = np.polyfit(t_min[:n_look],DF_median[:n_look],1)\n",
    "        poly_DFstd = np.polyfit(t_min[:n_look],DF_std[:n_look],1)\n",
    "        poly_DFskew = np.polyfit(t_min[:n_look],DF_skew[:n_look],1)\n",
    "        poly_DFkurt = np.polyfit(t_min[:n_look],DF_kurt[:n_look],1)\n",
    "    \n",
    "    # get normalized transmittance and PL at LD75\n",
    "    if LD75_exists:\n",
    "        Tr_failure = Tr_absolute[LD75_idx]\n",
    "        PL_failure = PLQY[LD75_idx]/PLQY[0]\n",
    "        PL_enhancement = np.mean(PLQY[:LD75_idx]/PLQY[0])\n",
    "    else:\n",
    "        Tr_failure = np.nan\n",
    "        PL_failure = np.nan\n",
    "        PL_enhancement = np.nan\n",
    "        \n",
    "    if LD80_cor_exists:\n",
    "        Tr_LD80 = Tr_absolute[LD80_cor_idx]\n",
    "        PL_LD80 = PLQY[LD80_cor_idx]/PLQY[0]\n",
    "        Abs_LD80 = Abs_norm[LD80_cor_idx]\n",
    "    else:\n",
    "        Tr_LD80 = np.nan\n",
    "        PL_LD80 = np.nan\n",
    "        Abs_LD80 = np.nan\n",
    "\n",
    "\n",
    "    # fill out data array\n",
    "    row_idx = valid_count-1\n",
    "    \n",
    "    feat_data[row_idx,0] = T\n",
    "    feat_data[row_idx,1] = RH\n",
    "    feat_data[row_idx,2] = pct_O2\n",
    "    feat_data[row_idx,3] = N_suns_stress\n",
    "    feat_data[row_idx,4] = MA_frac\n",
    "    feat_data[row_idx,5] = pBleach_rate\n",
    "    feat_data[row_idx,6] = bleach_rate_1_pct\n",
    "    feat_data[row_idx,7] = bleach_rate_2_pct\n",
    "    feat_data[row_idx,8] = bleach_rate_5_pct\n",
    "    feat_data[row_idx,9] = bleach_rate_fit_1_pct\n",
    "    feat_data[row_idx,10] = bleach_rate_fit_2_pct\n",
    "    feat_data[row_idx,11] = bleach_rate_fit_5_pct\n",
    "    feat_data[row_idx,12] = tLD75 \n",
    "    feat_data[row_idx,13] = dLDdt_0\n",
    "    feat_data[row_idx,14] = dPLdt_0\n",
    "    feat_data[row_idx,15] = LD[0]\n",
    "    feat_data[row_idx,16] = PLQY[0] \n",
    "    feat_data[row_idx,17] = xy1t0[0] \n",
    "    feat_data[row_idx,18] = xy0t1[0] \n",
    "    feat_data[row_idx,19] = xy0t1Norm[0] \n",
    "    feat_data[row_idx,20] = xy1t1[0] \n",
    "    feat_data[row_idx,21] = t0xy1[0] \n",
    "    feat_data[row_idx,22] = t1xy0[0] \n",
    "    feat_data[row_idx,23] = frac_bright[0]  \n",
    "    feat_data[row_idx,24] = ddTr0\n",
    "    feat_data[row_idx,25] = ddPL0\n",
    "    feat_data[row_idx,26] = ddLD0\n",
    "    feat_data[row_idx,27] = t_PLmax\n",
    "    feat_data[row_idx,28] = t_PLmax_10\n",
    "    feat_data[row_idx,29] = t_PL10\n",
    "    feat_data[row_idx,30] = Tr_failure\n",
    "    feat_data[row_idx,31] = PL_failure\n",
    "    feat_data[row_idx,32] = PL_enhancement\n",
    "    feat_data[row_idx,33] = pred_horiz\n",
    "    feat_data[row_idx,34:34+n_use] = LD_early\n",
    "    feat_data[row_idx,34+n_use:34+2*n_use] = Tr_early\n",
    "    feat_data[row_idx,34+2*n_use:34+3*n_use] = PL_early\n",
    "    feat_data[row_idx,34+3*n_use:34+4*n_use] = mu_early\n",
    "    feat_data[row_idx,34+4*n_use:34+5*n_use] = tau_early\n",
    "    feat_data[row_idx,34+5*n_use] = QFLS[0]\n",
    "    feat_data[row_idx,34+5*n_use+1] = tr_end\n",
    "    feat_data[row_idx,34+5*n_use+2] = Tr_finished\n",
    "    feat_sum = 34+5*n_use+2\n",
    "    feat_data[row_idx,feat_sum+1:feat_sum+6] = LD_1st_10\n",
    "    feat_data[row_idx,feat_sum+6:feat_sum+11] = PL_1st_10\n",
    "    feat_data[row_idx,feat_sum+11:feat_sum+16] = Tr_1st_10\n",
    "    feat_data[row_idx,feat_sum+16] = tLD80\n",
    "    feat_data[row_idx,feat_sum+17] = PL_LD80\n",
    "    feat_data[row_idx,feat_sum+18] = Tr_LD80\n",
    "    feat_data[row_idx,feat_sum+19] = dtaudt\n",
    "    feat_data[row_idx,feat_sum+20] = dmudt\n",
    "    feat_data[row_idx,feat_sum+21] = tau[0]\n",
    "    feat_data[row_idx,feat_sum+22] = mu[0]\n",
    "    feat_data[row_idx,feat_sum+23] = tLD75_cor\n",
    "    feat_data[row_idx,feat_sum+24] = tLD80_cor\n",
    "    feat_data[row_idx,feat_sum+25] = LD0_cor\n",
    "    feat_data[row_idx,feat_sum+26] = mu_LD80\n",
    "    feat_data[row_idx,feat_sum+27] = tau_LD80\n",
    "    feat_data[row_idx,feat_sum+28] = mu_LD75\n",
    "    feat_data[row_idx,feat_sum+29] = tau_LD75\n",
    "    feat_data[row_idx,feat_sum+30] = ddmu0\n",
    "    feat_data[row_idx,feat_sum+31] = ddtau0\n",
    "    feat_data[row_idx,feat_sum+32] = days_stored \n",
    "    feat_data[row_idx,feat_sum+33] = xy2t0[0] \n",
    "    feat_data[row_idx,feat_sum+34] = xy3t0[0] \n",
    "    feat_data[row_idx,feat_sum+35] = xy2t1[0]\n",
    "    feat_data[row_idx,feat_sum+36] = xy3t1[0] \n",
    "    feat_data[row_idx,feat_sum+37] = beta_mean[0]\n",
    "    feat_data[row_idx,feat_sum+38] = beta_std[0]\n",
    "    feat_data[row_idx,feat_sum+39] = cv_slopes[0] \n",
    "    feat_data[row_idx,feat_sum+40] = poly_DFmean[0]/DF_mean[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+41] = poly_DFmedian[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+42] = poly_DFstd[0]/DF_std[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+43] = poly_DFskew[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+44] = poly_DFkurt[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+45] = ddDFmean0/DF_mean[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+46] = ddDFmedian0 if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+47] = ddDFstd0/DF_std[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+48] = ddDFskew0 if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+49] = ddDFkurt0 if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+50] = DF_mean[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+51] = DF_median[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+52] = DF_std[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+53] = DF_skew[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+54] = DF_kurt[0] if dark_field else np.nan\n",
    "    feat_data[row_idx,feat_sum+55] = t_LDmax\n",
    "    feat_data[row_idx,feat_sum+56] = dAdt\n",
    "    feat_data[row_idx,feat_sum+57] = deg_rate\n",
    "    feat_data[row_idx,feat_sum+58] = ddA0\n",
    "    feat_data[row_idx,feat_sum+59] = vol_rate\n",
    "    feat_data[row_idx,feat_sum+60] = d_vol_rate\n",
    "    feat_data[row_idx,feat_sum+61] = photodiode_t0\n",
    "    feat_data[row_idx,feat_sum+62] = sigma_ph_0\n",
    "    feat_data[row_idx,feat_sum+63] = transmittance_t0\n",
    "    feat_data[row_idx,feat_sum+64] = reflectance_t0\n",
    "    feat_data[row_idx,feat_sum+65] = tA80\n",
    "    feat_data[row_idx,feat_sum+66] = Abs_LD80\n",
    "    \n",
    "\n",
    "# Assign factorial data to pandas DataFrame\n",
    "featdata_df = pd.DataFrame(data=feat_data,columns = ['Temp (deg C)','RH (%)','Oxygen (%)','Illum (Nsuns)','MA fraction',\n",
    "                                                   'Bleach Rate (polyfit) (1/min)','Bleach Rate (1% inc) (1/min)','Bleach Rate (2% inc) (1/min)','Bleach Rate (5% inc) (1/min)','Bleach Rate (fit to 1% inc) (1/min)','Bleach Rate (fit to 2% inc) (1/min)','Bleach Rate (fit to 5% inc) (1/min)',\n",
    "                                                   'tLD75 (min)','dLDdt (1/min)','dPLdt (1/min)','LD_0 (nm)','PLQY_0',\n",
    "                                                   'xy1t0_0','xy0t1_0','xy0t1Norm_0','xy1t1_0','t0xy1_0','t1xy0_0','frac_bright_0',\n",
    "                                                   'ddTr0','ddPL0','ddLD0','tPLmax','tPLmax10','tPL10','TrFail','PLFail','PLEnhance',\n",
    "                                                   'pred_horiz','Ld1','Ld2','Ld3',\n",
    "                                                   'Tr1','Tr2','Tr3',\n",
    "                                                   'PL1','PL2','PL3',\n",
    "                                                   'mu1','mu2','mu3',\n",
    "                                                   'tau1','tau2','tau3',\n",
    "                                                   'QFLS_0','Tr_end','Tr_finished',\n",
    "                                                   'Ld_2min','Ld_4min','Ld_6min','Ld_8min','Ld_10min',\n",
    "                                                   'PL_2min','PL_4min','PL_6min','PL_8min','PL_10min',\n",
    "                                                   'Tr_2min','Tr_4min','Tr_6min','Tr_8min','Tr_10min',\n",
    "                                                   'tLD80 (min)','PL_LD80','Tr_LD80',\n",
    "                                                   'dtaudt','dmudt','tau0','mu0',\n",
    "                                                   'tLD75 corrected (min)',\n",
    "                                                   'tLD80 corrected (min)',\n",
    "                                                   'LD_0 corrected (nm)',\n",
    "                                                   'mu_LD80','tau_LD80','mu_LD75','tau_LD75',\n",
    "                                                   'ddmu0','ddtau0','Days Stored',\n",
    "                                                   'xy2t0','xy3t0','xy2t1','xy3t1',\n",
    "                                                   'beta_mean','beta_std','cv_slopes',\n",
    "                                                   'dDFmeandt','dDFmediandt','dDFstddt','dDFskewdt','dDFkurtdt',\n",
    "                                                   'd2DFmeandt2','d2DFmediandt2','d2DFstddt2','d2DFskewdt2','d2DFkurtdt2',\n",
    "                                                   'DFmean_0','DFmedian_0','DFstd_0','DFskew_0','DFkurtdt_0',\n",
    "                                                   'tLDmax','dAdt','deg_rate','ddA0','vol_rate','d_vol_rate',\n",
    "                                                   'initial_PD_power','sigma_ph','Tr_0','Ref_0','tA80','Abs_LD80'\n",
    "                                                  ])\n",
    "\n",
    "            \n",
    "# add the class and experiment IDs at the start of the DF\n",
    "featdata_df.insert(0,'ClassID',ClassIDs)\n",
    "featdata_df.insert(1,'ExptID',ExptIDs)\n",
    "featdata_df.insert(2,'Film Thickness [nm]',Thicknesses)\n",
    "\n",
    "# view dataframe\n",
    "featdata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the complete dataframe to CSV\n",
    "savepath = ''\n",
    "savename = 'Film_Featurized_Data.csv'\n",
    "featdata_df.to_csv(savepath+savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
